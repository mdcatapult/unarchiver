akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "ERROR"
  stdout-loglevel = "ERROR"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
}

op-rabbit {
  topic-exchange-name = ${?RABBITMQ_EXCHANGE}
  channel-dispatcher = "op-rabbit.default-channel-dispatcher"
  default-channel-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-factor = 2.0
      parallelism-max = 4
    }
    throughput = 1
  }
  connection {
    virtual-host = "/"
    virtual-host = ${?RABBITMQ_VHOST}
    hosts = [${?RABBITMQ_HOST}]
    username = ${RABBITMQ_USERNAME}
    password = ${RABBITMQ_PASSWORD}
    port = 5672
    port = ${?RABBITMQ_PORT}
    ssl = false
    connection-timeout = 3s
  }
}

mongo {
  database = ${MONGO_DATABASE}
  collection = ${MONGO_COLLECTION}
  read-collection = ${mongo.collection}
  read-collection = ${?MONGO_COLLECTION_READ}
  write-collection = ${mongo.collection}
  write-collection = ${?MONGO_COLLECTION_WRITE}
  connection {
    hosts = [${MONGO_HOST}]
    username = ${MONGO_USERNAME}
    password = ${MONGO_PASSWORD}
    port = 27017
    port = ${?MONGO_PORT}
    database = "admin"
    database = ${?MONGO_AUTH_DB}
  }
}

updtream {
  concurrent: 1
  concurrent: ${?CONSUMER_CONCURRENT}
  queue: "doclib.unarchive"
  queue: ${?CONSUMER_QUEUE}
}

downstream {
  queue: "doclib.prefetch"
  queue: ${?PUBLISHER_QUEUE}
}

extract {
  to: {
    type: fs
    type: ${?EXTRACT_TO_TYPE}
    path: "./extracted"
    path: ${?EXTRACT_TO_PATH}
  }
}